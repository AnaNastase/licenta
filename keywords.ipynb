{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184c0c36-046b-408e-af72-815a97190459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import spacy\n",
    "import pyLDAvis.gensim_models\n",
    "import en_core_web_md\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "import yake\n",
    "from langdetect import detect, detect_langs, DetectorFactory\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c1abff3-4ab1-41c0-a7ab-41e6d12c0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "authors = pd.read_csv('top_20_authors.csv')\n",
    "publications = pd.read_csv('publications-top_20_authors.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f725448-49d4-451d-8590-888e07082835",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ro\n",
      "ro\n",
      "ro\n",
      "it\n",
      "it\n",
      "ro\n",
      "ro\n",
      "id\n",
      "id\n",
      "ro\n",
      "de\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "pt\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "fr\n",
      "fr\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "fr\n",
      "fr\n",
      "fr\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "fr\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "fr\n",
      "ro\n",
      "fr\n",
      "fr\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "fr\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "fr\n",
      "ro\n",
      "ro\n",
      "de\n",
      "de\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "fr\n",
      "ro\n",
      "ro\n",
      "ja\n",
      "ro\n",
      "ro\n",
      "fr\n",
      "es\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "de\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "fr\n",
      "ro\n",
      "et\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "et\n",
      "ro\n",
      "fr\n",
      "ro\n",
      "fr\n",
      "fr\n",
      "fr\n",
      "fr\n",
      "de\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "fr\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "pt\n",
      "pt\n",
      "ro\n",
      "ro\n",
      "fr\n",
      "et\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "tr\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "de\n",
      "de\n",
      "ro\n",
      "ro\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "ro\n",
      "de\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ca\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "fr\n",
      "es\n",
      "fr\n",
      "ro\n",
      "de\n",
      "de\n",
      "so\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "fr\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "fr\n",
      "fr\n",
      "fr\n",
      "fr\n",
      "fr\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "fr\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "de\n",
      "fr\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ca\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "fr\n",
      "fr\n",
      "ro\n",
      "ro\n",
      "fr\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "it\n",
      "ro\n",
      "fr\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "de\n",
      "fr\n"
     ]
    }
   ],
   "source": [
    "# make a dictionary containing the combined abstracts for each author\n",
    "author_publication_pairs = list(zip(publications['user_id'], publications['abstract_text']))\n",
    "\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "authors_texts = {author_id: \"\" for author_id in authors[\"id\"]}\n",
    "for author_id, abstract in author_publication_pairs:\n",
    "    if abstract and isinstance(abstract, str) and re.match('^(?=.*[a-zA-Z])', abstract):\n",
    "        language = detect(abstract)\n",
    "        if language != 'en':\n",
    "            print(language)\n",
    "        else:\n",
    "            authors_texts[author_id] = authors_texts[author_id] + \"\\n\" + abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "548fe614-5925-48a2-bd17-1a6efd289e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd89355-9868-42e9-8038-4f9e98a776f8",
   "metadata": {},
   "source": [
    "EXTRACT KEYWORDS/KEYPHRASES WITH YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0425d20e-4646-4d08-b0a1-35b791136696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('distributed systems', 3.509302360162876e-05)\n",
      "('Mobile Cloud computing', 3.809210395500882e-05)\n",
      "('cloud computing systems', 5.410482182189355e-05)\n",
      "('mobile devices', 8.652648328096323e-05)\n",
      "('Opportunistic networks', 9.303329906713266e-05)\n",
      "('network management services', 0.00015152061105763796)\n",
      "('mobile social networks', 0.00015433899488251414)\n",
      "('systems', 0.00015734145430671892)\n",
      "('network', 0.00019232239228677731)\n",
      "('mobile opportunistic cloud', 0.00020049293754866143)\n",
      "('Services', 0.00023050105690822903)\n",
      "('mobile', 0.00023288249418062875)\n",
      "('intelligent transportation systems', 0.0002460307080771078)\n",
      "('service computing architectures', 0.0002676603830539024)\n",
      "('Big Data', 0.00026862447805840105)\n"
     ]
    }
   ],
   "source": [
    "kw_extractor = yake.KeywordExtractor()\n",
    "max_ngram = 3\n",
    "deduplication_threshold = 0.5\n",
    "keywords_nr = 15\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=\"en\", n=max_ngram, dedupLim=deduplication_threshold, top=keywords_nr, features=None)\n",
    "\n",
    "text = authors_texts[534]\n",
    "doc = nlp(text)\n",
    "remove_entities = ['PERSON', 'NORP', 'FAC', 'GPE', 'LOC', 'DATE', 'TIME', 'PERCENT', 'MONEY',\n",
    "                   'QUANTITY', 'CARDINAL', 'ORDINAL']\n",
    "stop_words = ['paper', 'present', 'propose', 'show', 'datum', 'people', 'result', 'solution', 'case', 'order',\n",
    "                  'base', 'ieee', 'privacy', 'policy', 'new', 'old', 'context', 'high', 'different', 'research', 'type',\n",
    "                  'approach', 'important', 'main', 'range', 'helpful', 'large', 'difficult', 'available', 'amount',\n",
    "                  'useful', 'importance', 'article', 'abstract', 'scale', 'copyright', 'real', 'quality', 'demonstrate',\n",
    "                  'inconvenience', 'benefit', 'unavailable', 'term', 'condition', 'interest', 'recent', 'obtain',\n",
    "                  'organization', 'task', 'student', 'professor', 'teacher', 'university', 'workshop', 'study', 'text', 'jat', 'jats']\n",
    "\n",
    "transformed_text = ' '.join([token.text for token in doc if token.ent_type_ not in remove_entities\n",
    "                             and token.lemma_.lower() not in stop_words])\n",
    "\n",
    "keywords = custom_kw_extractor.extract_keywords(transformed_text)\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6547576-9eb8-41b3-a33d-56ca6e782748",
   "metadata": {},
   "source": [
    "EXTRACT KEYWORDS/KEYPHRASES WITH YAKE - from each abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c056608-c34c-4163-b837-645d3bb793de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary containing a list of abstracts for each author\n",
    "author_publication_pairs = list(zip(publications['user_id'], publications['abstract_text']))\n",
    "\n",
    "authors_texts = {author_id:[] for author_id in authors[\"id\"]}\n",
    "for author_id, abstract in author_publication_pairs:\n",
    "    if abstract and isinstance(abstract, str) and re.match('^(?=.*[a-zA-Z])', abstract):\n",
    "        language = detect(abstract)\n",
    "        if language == 'en':\n",
    "            authors_texts[author_id].append(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "17cbef0a-9228-462b-bbd1-83b1ff8f70f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate chunks of 25 abstracts\n",
    "chunk_size = 100\n",
    "abstracts = authors_texts[534]\n",
    "new_abstracts = []\n",
    "abstract_count = len(abstracts)\n",
    "\n",
    "for i in range(0, abstract_count, chunk_size):\n",
    "    end = i + chunk_size if i + 2 * chunk_size <= abstract_count else abstract_count\n",
    "    abstracts_chunk = ' '.join(abstracts[i:i+chunk_size])\n",
    "    new_abstracts.append(abstracts_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5922013e-1441-41c3-bcf3-1252e3b97fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('distributed systems', 0.00014737726273259392)\n",
      "('Mobile Cloud computing', 0.0001553843845553562)\n",
      "('CPC Program Library', 0.00019293281743546688)\n",
      "('distributed system technologies', 0.00021163232168504875)\n",
      "('Opportunistic Mobile Networks', 0.00022045292001703065)\n",
      "\n",
      "('distributed systems', 0.00010278648299842698)\n",
      "('mobile cloud computing', 0.00013423850838873276)\n",
      "('mobile cloud systems', 0.00015288315547572217)\n",
      "('Cloud computing systems', 0.00020420207726288753)\n",
      "('distributed system technologies', 0.0002947597311836742)\n",
      "\n",
      "('Intelligent Transportation Systems', 0.00031589866062634497)\n",
      "('opportunistic networks', 0.00035659915944817806)\n",
      "('network management services', 0.0003662044185913426)\n",
      "('distributed systems', 0.0004116821210382379)\n",
      "('Opportunistic network applications', 0.0004933637909109081)\n",
      "\n",
      "('technical professional dedicated', 5.537009241867369e-05)\n",
      "('web site signifies', 9.52271798327684e-05)\n",
      "('advancing technology', 0.0002662045235255443)\n",
      "('distributed systems', 0.0003710776392339499)\n",
      "('Single sales', 0.0004201189358535599)\n",
      "\n",
      "('Vrije Universiteit Castiglione', 0.00016936920614667238)\n",
      "('Alpen Adria Klagenfurt', 0.00032565315101506495)\n",
      "('Klagenfurt Florin Pop', 0.00032565315101506495)\n",
      "('Adria Klagenfurt Florin', 0.00032599663428835345)\n",
      "('Salerno Kubilay Atasu', 0.0004054429189210268)\n",
      "\n",
      "('math xmlns', 6.743712824339385e-05)\n",
      "('mml', 7.988065670962036e-05)\n",
      "('Standard Model', 0.00025660045632840784)\n",
      "('math', 0.0003077226656069247)\n",
      "('NLO Monte Carlo', 0.00032472333789818476)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kw_extractor = yake.KeywordExtractor()\n",
    "max_ngram = 3\n",
    "deduplication_threshold = 0.7\n",
    "keywords_nr = 5\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=\"en\", n=max_ngram, dedupLim=deduplication_threshold, top=keywords_nr, features=None)\n",
    "\n",
    "remove_entities = ['PERSON', 'NORP', 'FAC', 'GPE', 'LOC', 'DATE', 'TIME', 'PERCENT', 'MONEY',\n",
    "                   'QUANTITY', 'CARDINAL', 'ORDINAL']\n",
    "stop_words = ['paper', 'present', 'propose', 'show', 'datum', 'people', 'result', 'solution', 'case', 'order',\n",
    "                  'base', 'ieee', 'privacy', 'policy', 'new', 'old', 'context', 'high', 'different', 'research', 'type',\n",
    "                  'approach', 'important', 'main', 'range', 'helpful', 'large', 'difficult', 'available', 'amount',\n",
    "                  'useful', 'importance', 'article', 'abstract', 'scale', 'copyright', 'real', 'quality', 'demonstrate',\n",
    "                  'inconvenience', 'benefit', 'unavailable', 'term', 'condition', 'interest', 'recent', 'obtain',\n",
    "                  'organization', 'task', 'student', 'professor', 'teacher', 'university', 'workshop', 'study', 'text', 'jat', 'jats']\n",
    "\n",
    "for text in new_abstracts:\n",
    "    doc = nlp(text)\n",
    "    transformed_text = ' '.join([token.text for token in doc if token.ent_type_ not in remove_entities\n",
    "                             and token.lemma_.lower() not in stop_words])\n",
    "    \n",
    "    keywords = custom_kw_extractor.extract_keywords(transformed_text)\n",
    "    for kw in keywords:\n",
    "        print(kw)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b48cf0-57ee-4704-b7bc-3b79fe6fe1da",
   "metadata": {},
   "source": [
    "EXTRACT TOPICS WITH GENSIM LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "60935350-c225-4c74-b2d6-c5a5a4697f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary containing a list of abstracts for each author\n",
    "author_publication_pairs = list(zip(publications['user_id'], publications['abstract_text']))\n",
    "\n",
    "authors_texts = {author_id:[] for author_id in authors[\"id\"]}\n",
    "for author_id, abstract in author_publication_pairs:\n",
    "    if abstract and isinstance(abstract, str) and re.match('^(?=.*[a-zA-Z])', abstract):\n",
    "        language = detect(abstract)\n",
    "        if language == 'en':\n",
    "            authors_texts[author_id].append(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "72097b7e-9438-46c0-a8f7-718e74e8c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = authors_texts[829]\n",
    "\n",
    "remove_pos = ['ADV', 'PRON', 'PART', 'DET', 'SPACE', 'NUM', 'SYM', 'ADP', 'VERB', 'CCONJ']\n",
    "stop_words = ['paper', 'present', 'propose', 'datum', 'people', 'result', 'solution', 'case', 'order', 'base', 'ieee', 'privacy', 'policy',\n",
    "             'new', 'old', 'context', 'high', 'different', 'new', 'old', 'research', 'type', 'approach', 'important', 'main', 'range',\n",
    "             'helpful', 'large', 'difficult', 'available', 'amount', 'useful', 'importance', 'article', 'abstract', 'scale', 'copyright',\n",
    "             'real', 'quality', 'inconvenience', 'benefit', 'unavailable', 'term', 'condition', 'interest', 'organization', 'use',\n",
    "             'task', 'student', 'professor', 'teacher', 'university', 'conference', 'thank']\n",
    "\n",
    "# keep only adjectives and nouns\n",
    "remove_entities = ['PERSON', 'NORP', 'FAC', 'GPE', 'LOC', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY',\n",
    "                   'QUANTITY', 'CARDINAL', 'ORDINAL']\n",
    "\n",
    "tokens = []\n",
    "for abstract in texts:\n",
    "    doc = nlp(abstract)\n",
    "    t = [token.lemma_.lower() for token in doc if token.is_alpha and token.ent_type_ not in remove_entities\n",
    "                             and token.lemma_.lower() not in stop_words and token.pos_ not in remove_pos and not token.is_stop]\n",
    "    tokens.append(t)\n",
    "\n",
    "# add bigrams to the token list\n",
    "bigram = Phrases(tokens, min_count=2, delimiter=' ', threshold=1)\n",
    "tokens = [bigram[text] for text in tokens]\n",
    "trigram = Phrases(tokens, min_count=2, delimiter=' ', threshold=1)\n",
    "tokens = [trigram[text] for text in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "828a68b9-b9bb-4b28-b663-b8ee92c11d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove single words\n",
    "tokens = [[token for token in text if len(token.split(\" \")) > 1] for text in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "13270681-56fc-42e1-b8ec-a93f8e6053cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with gensim\n",
    "dictionary = Dictionary(tokens)\n",
    "# create corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c3b75514-81ff-4259-99bb-3bb2e5dff765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013*\"sar image\" + 0.008*\"synthetic aperture radar sar\" + 0.008*\"satellite image\" + 0.005*\"earth observation\" + 0.005*\"feature extraction\" + 0.005*\"image content\" + 0.004*\"terrasar x\" + 0.004*\"image patch\" + 0.003*\"land cover\" + 0.003*\"remote image\"\n"
     ]
    }
   ],
   "source": [
    "# apply LDA\n",
    "lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=300, num_topics=1, workers=4, passes=50)\n",
    "topics = lda_model.print_topics(-1)\n",
    "\n",
    "for idx, topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa39a1-5fb5-4a3f-bde8-f501188b3719",
   "metadata": {},
   "source": [
    "EXTRACT KEYWORDS FROM PUBLICATION TITLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0db870d0-d729-4a33-b9d2-f25dad4908f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary containing the combined titles for each author\n",
    "author_title_pairs = list(zip(publications['user_id'], publications['title']))\n",
    "\n",
    "authors_titles = {author_id:\"\" for author_id in authors[\"id\"]}\n",
    "for author_id, title in author_title_pairs:\n",
    "    if title and isinstance(title, str):\n",
    "        authors_titles[author_id] += \"\\n\" + title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b8ff1fb9-f433-465e-9ae5-71d02ff085d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Cloud Computing', 3.0745344837092404e-06)\n",
      "('Big Data', 3.2118868621537734e-06)\n",
      "('distributed systems', 3.458271355126227e-06)\n"
     ]
    }
   ],
   "source": [
    "kw_extractor = yake.KeywordExtractor()\n",
    "max_ngram = 2\n",
    "deduplication_threshold = 0.2\n",
    "keywords_nr = 3\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=\"en\", n=max_ngram, dedupLim=deduplication_threshold, top=keywords_nr, features=None)\n",
    "\n",
    "text = authors_titles[562]\n",
    "doc = nlp(text)\n",
    "\n",
    "transformed_text = ' '.join([token.text for token in doc])\n",
    "\n",
    "keywords = custom_kw_extractor.extract_keywords(transformed_text)\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d94b9-a0e0-4fe6-9601-8e7e4e7c8d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
