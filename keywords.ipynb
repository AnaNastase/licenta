{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "184c0c36-046b-408e-af72-815a97190459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import spacy\n",
    "import pyLDAvis.gensim_models\n",
    "import en_core_web_md\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "import yake\n",
    "from langdetect import detect, detect_langs, DetectorFactory\n",
    "import fasttext\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c1abff3-4ab1-41c0-a7ab-41e6d12c0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "authors = pd.read_csv('top_20_authors.csv')\n",
    "publications = pd.read_csv('publications-top_20_authors.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f725448-49d4-451d-8590-888e07082835",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# make a dictionary containing the combined abstracts for each author\n",
    "author_publication_pairs = list(zip(publications['user_id'], publications['abstract_text']))\n",
    "\n",
    "# load fastText model\n",
    "model = fasttext.load_model('lid.176.bin')\n",
    "\n",
    "authors_texts = {author_id: \"\" for author_id in authors[\"id\"]}\n",
    "for author_id, abstract in author_publication_pairs:\n",
    "    if abstract and isinstance(abstract, str) and re.match('^(?=.*[a-zA-Z])', abstract):\n",
    "        predictions = model.predict(abstract)\n",
    "        language = predictions[0][0].replace('__label__', '')\n",
    "        if language == 'en':\n",
    "            authors_texts[author_id] = authors_texts[author_id] + '\\n' + abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "548fe614-5925-48a2-bd17-1a6efd289e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae1c8504-e92c-4ea9-8a54-7d5de6a11637",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['paper', 'present', 'propose', 'show', 'datum', 'people', 'result', 'solution', 'case', 'order',\n",
    "              'base', 'ieee', 'privacy', 'policy', 'new', 'old', 'context', 'high', 'different', 'research', 'type',\n",
    "              'approach', 'important', 'main', 'range', 'helpful', 'large', 'difficult', 'available', 'amount',\n",
    "              'useful', 'importance', 'article', 'abstract', 'scale', 'copyright', 'real', 'quality', 'demonstrate',\n",
    "              'inconvenience', 'benefit', 'unavailable', 'term', 'condition', 'interest', 'recent', 'obtain',\n",
    "              'title', 'jat', 'jats',\n",
    "              'organization', 'task', 'student', 'professor', 'teacher', 'university', 'workshop', 'study', 'text',\n",
    "              'conference']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd89355-9868-42e9-8038-4f9e98a776f8",
   "metadata": {},
   "source": [
    "EXTRACT KEYWORDS/KEYPHRASES WITH YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0425d20e-4646-4d08-b0a1-35b791136696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('distributed systems', 3.4364722657217926e-05)\n",
      "('Mobile Cloud computing', 3.725239951607386e-05)\n",
      "('cloud computing systems', 5.28856534941652e-05)\n",
      "('mobile devices', 8.456675262873203e-05)\n",
      "('Opportunistic networks', 9.144074241533435e-05)\n",
      "('network management services', 0.00014725209553199654)\n",
      "('mobile social networks', 0.00014988644505110576)\n",
      "('systems', 0.00015569770970941547)\n",
      "('network', 0.00019029201586369998)\n",
      "('mobile opportunistic cloud', 0.00019605864499052693)\n",
      "('Services', 0.00022863763957386274)\n",
      "('mobile', 0.00023057430805365037)\n",
      "('intelligent transportation systems', 0.00024168145356403626)\n",
      "('service computing architectures', 0.0002616473841634621)\n",
      "('distributed network services', 0.00026547025450600136)\n"
     ]
    }
   ],
   "source": [
    "kw_extractor = yake.KeywordExtractor()\n",
    "max_ngram = 3\n",
    "deduplication_threshold = 0.5\n",
    "keywords_nr = 15\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=\"en\", n=max_ngram, dedupLim=deduplication_threshold, top=keywords_nr, features=None)\n",
    "\n",
    "text = authors_texts[534]\n",
    "doc = nlp(text)\n",
    "remove_entities = ['PERSON', 'NORP', 'FAC', 'GPE', 'LOC', 'DATE', 'TIME', 'PERCENT', 'MONEY',\n",
    "                   'QUANTITY', 'CARDINAL', 'ORDINAL']\n",
    "\n",
    "transformed_text = ' '.join([token.text for token in doc if token.ent_type_ not in remove_entities\n",
    "                             and token.lemma_.lower() not in stop_words])\n",
    "\n",
    "keywords = custom_kw_extractor.extract_keywords(transformed_text)\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6547576-9eb8-41b3-a33d-56ca6e782748",
   "metadata": {},
   "source": [
    "EXTRACT KEYWORDS/KEYPHRASES WITH YAKE - from each abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c056608-c34c-4163-b837-645d3bb793de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary containing a list of abstracts for each author\n",
    "author_publication_pairs = list(zip(publications['user_id'], publications['abstract_text']))\n",
    "\n",
    "authors_texts = {author_id:[] for author_id in authors[\"id\"]}\n",
    "for author_id, abstract in author_publication_pairs:\n",
    "    if abstract and isinstance(abstract, str) and re.match('^(?=.*[a-zA-Z])', abstract):\n",
    "        language = detect(abstract)\n",
    "        if language == 'en':\n",
    "            authors_texts[author_id].append(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17cbef0a-9228-462b-bbd1-83b1ff8f70f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate chunks of 25 abstracts\n",
    "chunk_size = 100\n",
    "abstracts = authors_texts[534]\n",
    "new_abstracts = []\n",
    "abstract_count = len(abstracts)\n",
    "\n",
    "for i in range(0, abstract_count, chunk_size):\n",
    "    end = i + chunk_size if i + 2 * chunk_size <= abstract_count else abstract_count\n",
    "    abstracts_chunk = ' '.join(abstracts[i:i+chunk_size])\n",
    "    new_abstracts.append(abstracts_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5922013e-1441-41c3-bcf3-1252e3b97fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('distributed systems', 0.00014751986914513155)\n",
      "('Mobile Cloud computing', 0.0001555608069588123)\n",
      "('CPC Program Library', 0.0001928830969001555)\n",
      "('distributed system technologies', 0.00021192143569025227)\n",
      "('Opportunistic Mobile Networks', 0.00022074089837374123)\n",
      "\n",
      "('distributed systems', 0.00010283178410928555)\n",
      "('mobile cloud computing', 0.00013430140315475136)\n",
      "('mobile cloud systems', 0.00015297031532179986)\n",
      "('Cloud computing systems', 0.00020430597772334866)\n",
      "('distributed system technologies', 0.0002949470001309614)\n",
      "\n",
      "('Intelligent Transportation Systems', 0.0003160180608889025)\n",
      "('opportunistic networks', 0.0003567229004126022)\n",
      "('network management services', 0.00036641204050500255)\n",
      "('distributed systems', 0.00041184666832745853)\n",
      "('Opportunistic network applications', 0.0004936438126448163)\n",
      "\n",
      "('technical professional dedicated', 5.470772391643552e-05)\n",
      "('web site signifies', 9.401472685159501e-05)\n",
      "('advancing technology', 0.00026436612094685337)\n",
      "('distributed systems', 0.0003682250908961957)\n",
      "('Single sales', 0.0004176350535979555)\n",
      "\n",
      "('Vrije Universiteit Castiglione', 0.0001693865993325655)\n",
      "('Alpen Adria Klagenfurt', 0.00032567788499400914)\n",
      "('Klagenfurt Florin Pop', 0.00032567788499400914)\n",
      "('Adria Klagenfurt Florin', 0.00032602138827275283)\n",
      "('Salerno Kubilay Atasu', 0.00040546859382384267)\n",
      "\n",
      "('math xmlns', 6.740271220820608e-05)\n",
      "('mml', 7.985931579339566e-05)\n",
      "('Standard Model', 0.00025653854542350573)\n",
      "('math', 0.00030764113590233685)\n",
      "('NLO Monte Carlo', 0.00032469552609883497)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kw_extractor = yake.KeywordExtractor()\n",
    "max_ngram = 3\n",
    "deduplication_threshold = 0.7\n",
    "keywords_nr = 5\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=\"en\", n=max_ngram, dedupLim=deduplication_threshold, top=keywords_nr, features=None)\n",
    "\n",
    "remove_entities = ['PERSON', 'NORP', 'FAC', 'GPE', 'LOC', 'DATE', 'TIME', 'PERCENT', 'MONEY',\n",
    "                   'QUANTITY', 'CARDINAL', 'ORDINAL']\n",
    "\n",
    "for text in new_abstracts:\n",
    "    doc = nlp(text)\n",
    "    transformed_text = ' '.join([token.text for token in doc if token.ent_type_ not in remove_entities\n",
    "                             and token.lemma_.lower() not in stop_words])\n",
    "    \n",
    "    keywords = custom_kw_extractor.extract_keywords(transformed_text)\n",
    "    for kw in keywords:\n",
    "        print(kw)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b48cf0-57ee-4704-b7bc-3b79fe6fe1da",
   "metadata": {},
   "source": [
    "EXTRACT TOPICS WITH GENSIM LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60935350-c225-4c74-b2d6-c5a5a4697f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary containing a list of abstracts for each author\n",
    "author_publication_pairs = list(zip(publications['user_id'], publications['abstract_text']))\n",
    "\n",
    "authors_texts = {author_id:[] for author_id in authors[\"id\"]}\n",
    "for author_id, abstract in author_publication_pairs:\n",
    "    if abstract and isinstance(abstract, str) and re.match('^(?=.*[a-zA-Z])', abstract):\n",
    "        language = detect(abstract)\n",
    "        if language == 'en':\n",
    "            authors_texts[author_id].append(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72097b7e-9438-46c0-a8f7-718e74e8c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = authors_texts[829]\n",
    "\n",
    "# keep only adjectives and nouns\n",
    "remove_pos = ['ADV', 'PRON', 'PART', 'DET', 'SPACE', 'NUM', 'SYM', 'ADP', 'VERB', 'CCONJ']\n",
    "remove_entities = ['PERSON', 'NORP', 'FAC', 'GPE', 'LOC', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY',\n",
    "                   'QUANTITY', 'CARDINAL', 'ORDINAL']\n",
    "\n",
    "tokens = []\n",
    "for abstract in texts:\n",
    "    doc = nlp(abstract)\n",
    "    t = [token.lemma_.lower() for token in doc if token.is_alpha and token.ent_type_ not in remove_entities\n",
    "                             and token.lemma_.lower() not in stop_words and token.pos_ not in remove_pos and not token.is_stop]\n",
    "    tokens.append(t)\n",
    "\n",
    "# add bigrams to the token list\n",
    "bigram = Phrases(tokens, min_count=2, delimiter=' ', threshold=1)\n",
    "tokens = [bigram[text] for text in tokens]\n",
    "trigram = Phrases(tokens, min_count=2, delimiter=' ', threshold=1)\n",
    "tokens = [trigram[text] for text in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a68b9-b9bb-4b28-b663-b8ee92c11d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove single words\n",
    "tokens = [[token for token in text if len(token.split(\" \")) > 1] for text in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13270681-56fc-42e1-b8ec-a93f8e6053cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with gensim\n",
    "dictionary = Dictionary(tokens)\n",
    "# create corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b75514-81ff-4259-99bb-3bb2e5dff765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply LDA\n",
    "lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=300, num_topics=1, workers=4, passes=50)\n",
    "topics = lda_model.print_topics(-1)\n",
    "\n",
    "for idx, topic in topics:\n",
    "    print(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
